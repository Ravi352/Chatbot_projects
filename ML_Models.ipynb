{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOss2+D8KlQ2U+OSb8DquWj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install tqdm\n","!python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8WRpr7CUrtf3","executionInfo":{"status":"ok","timestamp":1693378703920,"user_tz":-330,"elapsed":48176,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"6ab5e297-b344-459c-ae91-ff15f2d31a88"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n","2023-08-30 06:58:03.373177: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-30 06:58:05.250185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","2023-08-30 06:58:08.491783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-08-30 06:58:08.492433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","2023-08-30 06:58:08.492683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n","Collecting en-core-web-sm==3.6.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n","Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.6.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","import re\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('word_tokenize')\n","nltk.download('WordNetLemmatizer')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","# Step 1: Load and preprocess your dataset\n","# Assuming you have a CSV file with 'text' and 'intent' columns\n","data = pd.read_csv('/content/output_transformed.csv')\n","data = data.dropna()\n","# Step 2: Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.2, random_state=42)\n","\n","# Step 3: Feature extraction using TF-IDF\n","tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Step 4: Train a classification model (Multinomial Naive Bayes)\n","clf = MultinomialNB()\n","clf.fit(X_train_tfidf, y_train)\n","\n","# Step 5: Make predictions on the test data\n","y_pred = clf.predict(X_test_tfidf)\n","\n","# Step 6: Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy}')\n","\n","# Print a classification report with precision, recall, and F1-score\n","# print(classification_report(y_test, y_pred))\n","\n","def preprocess_text(text):\n","    # Remove special characters and digits\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Lemmatize the words\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Rejoin the tokens into a single string\n","    preprocessed_text = ' '.join(tokens)\n","\n","    return preprocessed_text\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYsJFmOuoELs","executionInfo":{"status":"ok","timestamp":1693378705963,"user_tz":-330,"elapsed":2050,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"cd41a147-fe92-4d9b-e073-f26a32fdd587"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.49029126213592233\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Error loading word_tokenize: Package 'word_tokenize' not\n","[nltk_data]     found in index\n","[nltk_data] Error loading WordNetLemmatizer: Package\n","[nltk_data]     'WordNetLemmatizer' not found in index\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["# Preprocess the new text\n","new_text = \"Can you explain me in one sentence what you are doing?\"\n","new_text_preprocessed = preprocess_text(new_text)  # Apply the same preprocessing steps\n","\n","# Feature extraction using the same TF-IDF vectorizer\n","new_text_tfidf = tfidf_vectorizer.transform([new_text_preprocessed])\n","\n","# Predict the intent label\n","predicted_intent = clf.predict(new_text_tfidf)[0]\n","\n","print(f\"Predicted Intent: {predicted_intent}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1HgqkfOZoKWB","executionInfo":{"status":"ok","timestamp":1693378709011,"user_tz":-330,"elapsed":3057,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"d128a303-041b-4adf-a0fa-3d62bc533941"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Intent: out_of_scope\n"]}]},{"cell_type":"code","source":["import spacy\n","import en_core_web_sm\n","from sklearn.preprocessing import MinMaxScaler #fixed import\n","nlp = en_core_web_sm.load()\n","# Load spaCy with pre-trained GloVe embeddings\n","# nlp = spacy.load('en_core_web_md')\n","\n","data = pd.read_csv('/content/output_transformed.csv')\n","data = data.dropna()\n","\n","X_train, X_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.2, random_state=42)\n","\n","def text_to_vector(text):\n","    doc = nlp(text)\n","    vector = doc.vector\n","    return vector\n","\n","# Vectorize your text data\n","X_train_vectors = [text_to_vector(text) for text in X_train]\n","X_test_vectors = [text_to_vector(text) for text in X_test]\n","\n","scaler = MinMaxScaler()\n","X_train_vectors = scaler.fit_transform(X_train_vectors)\n","X_test_vectors = scaler.transform(X_test_vectors)\n","\n","# Step 4: Train a classification model (Multinomial Naive Bayes)\n","clf = MultinomialNB()\n","clf.fit(X_train_vectors, y_train)\n","\n","# Step 5: Make predictions on the test data\n","y_pred = clf.predict(X_test_vectors)\n","\n","# Step 6: Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy}')\n","\n","# Print a classification report with precision, recall, and F1-score\n","# print(classification_report(y_test, y_pred))\n","\n","\n","\n","def preprocess_text(text):\n","    # Remove special characters and digits\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Lemmatize the words\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Rejoin the tokens into a single string\n","    preprocessed_text = ' '.join(tokens)\n","\n","    return preprocessed_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"id":"COrLBjOSkUtB","executionInfo":{"status":"error","timestamp":1693394376013,"user_tz":-330,"elapsed":12765,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"1f419c33-4be4-4feb-a86f-ef10ae2ab516"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6436aaa25f6a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# nlp = spacy.load('en_core_web_md')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/output_transformed.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_vectors = scaler.fit_transform(X_train_vectors)\n","X_test_vectors = scaler.transform(X_test_vectors)\n","from sklearn.svm import SVC\n","\n","svm_clf = SVC(kernel='linear')  # You can try different kernels\n","svm_clf.fit(X_train_vectors, y_train)\n","\n","# Predict and evaluate as before\n","y_pred_svm = svm_clf.predict(X_test_vectors)\n","accuracy_svm = accuracy_score(y_test, y_pred_svm)\n","print(f'SVM Accuracy: {accuracy_svm}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0RFZ232YvKCd","executionInfo":{"status":"ok","timestamp":1693378731152,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"fa27c834-5235-411e-dabf-467d85e34edb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Accuracy: 0.6019417475728155\n"]}]},{"cell_type":"code","source":["scaler = StandardScaler()\n","X_train_vectors = scaler.fit_transform(X_train_vectors)\n","X_test_vectors = scaler.transform(X_test_vectors)\n","from sklearn.svm import SVC\n","svm_clf = SVC(kernel='rbf', gamma='scale')  # You can try different kernels\n","svm_clf.fit(X_train_vectors, y_train)\n","\n","# Predict and evaluate as before\n","y_pred_svm = svm_clf.predict(X_test_vectors)\n","accuracy_svm = accuracy_score(y_test, y_pred_svm)\n","print(f'SVM Accuracy: {accuracy_svm}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PNgGxX9ywZBD","executionInfo":{"status":"ok","timestamp":1693378731820,"user_tz":-330,"elapsed":674,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"878d2f3f-01be-4d6b-f81c-b7387cc0f0a6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Accuracy: 0.5631067961165048\n"]}]},{"cell_type":"code","source":["scaler = StandardScaler()\n","X_train_vectors = scaler.fit_transform(X_train_vectors)\n","X_test_vectors = scaler.transform(X_test_vectors)\n","from sklearn.svm import SVC\n","svm_clf = SVC(kernel='poly', degree=1)  # You can try different kernels\n","svm_clf.fit(X_train_vectors, y_train)\n","\n","# Predict and evaluate as before\n","y_pred_svm = svm_clf.predict(X_test_vectors)\n","accuracy_svm = accuracy_score(y_test, y_pred_svm)\n","print(f'SVM Accuracy: {accuracy_svm}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCL-oFBUw7kq","executionInfo":{"status":"ok","timestamp":1693378731821,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"57bf496e-a042-4ad4-ae3d-3c46954941cb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Accuracy: 0.5\n"]}]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","\n","# Load the BERT tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","# Tokenize and obtain BERT embeddings\n","def get_bert_embeddings(text):\n","    input_ids = tokenizer.encode(text, add_special_tokens=True)\n","    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Batch size of 1\n","    with torch.no_grad():\n","        outputs = model(input_ids)\n","        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()  # Average the embeddings\n","    return embeddings\n","\n","# Vectorize your text data using BERT embeddings\n","X_train_bert = [get_bert_embeddings(text) for text in X_train]\n","X_test_bert = [get_bert_embeddings(text) for text in X_test]\n"],"metadata":{"id":"H1E3hdSrwMiH","executionInfo":{"status":"ok","timestamp":1693378972154,"user_tz":-330,"elapsed":123792,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!pip3 install \"scikit_learn==0.22.2.post1\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kDm3EVI6jbK","executionInfo":{"status":"ok","timestamp":1693378838038,"user_tz":-330,"elapsed":11030,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"b00b9ce1-0486-4e0c-fff5-23344b17329c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit_learn==0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==0.22.2.post1) (1.23.5)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==0.22.2.post1) (1.10.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit_learn==0.22.2.post1) (1.3.2)\n"]}]},{"cell_type":"code","source":["from sklearn.linear_model  import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","# Train a logistic regression classifier\n","scaler = StandardScaler()\n","X_train_bert = scaler.fit_transform(X_train_bert)\n","X_test_bert = scaler.transform(X_test_bert)\n","\n","LR_clf = LogisticRegression(max_iter=1000)\n","LR_clf.fit(X_train_bert, y_train)\n","\n","# Make predictions\n","y_pred = LR_clf.predict(X_test_bert)\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","# report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")"],"metadata":{"id":"ux0BsSbywRrn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693378986927,"user_tz":-330,"elapsed":9415,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"acc65a4d-5361-4ad0-9ba8-e83f41e12552"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7669902912621359\n"]}]},{"cell_type":"code","source":["## decision Tree\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","# Create and train a Decision Tree model\n","scaler = StandardScaler()\n","X_train_bert = scaler.fit_transform(X_train_bert)\n","X_test_bert = scaler.transform(X_test_bert)\n","DT_clf = DecisionTreeClassifier()\n","DT_clf.fit(X_train_bert, y_train)\n","\n","# Make predictions\n","y_pred = DT_clf.predict(X_test_bert)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","# report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n"],"metadata":{"id":"vNYAjWcnyLjw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693378990811,"user_tz":-330,"elapsed":959,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"2f3917b0-93df-462f-f8d6-675c3d8bded8"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.38349514563106796\n"]}]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","scaler = StandardScaler()\n","X_train_bert = scaler.fit_transform(X_train_bert)\n","X_test_bert = scaler.transform(X_test_bert)\n","RF_clf = RandomForestClassifier()\n","RF_clf.fit(X_train_bert, y_train)\n","\n","# Make predictions\n","y_pred = RF_clf.predict(X_test_bert)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","# report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n"],"metadata":{"id":"JTqRpq-ezm1g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693378997920,"user_tz":-330,"elapsed":3506,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"81cc6999-3c7f-4c80-9b30-a87e470ab780"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.587378640776699\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","scaler = StandardScaler()\n","X_train_bert = scaler.fit_transform(X_train_bert)\n","X_test_bert = scaler.transform(X_test_bert)\n","\n","SVC_clf = SVC(kernel='rbf', gamma='scale')\n","SVC_clf.fit(X_train_bert, y_train)\n","\n","# Make predictions\n","y_pred = SVC_clf.predict(X_test_bert)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n"],"metadata":{"id":"KSkmwsHUz3Lo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693379004071,"user_tz":-330,"elapsed":2455,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"72e2b1fe-6c48-443b-9684-49afcf169d8d"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.6941747572815534\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","scaler = StandardScaler()\n","X_train_bert = scaler.fit_transform(X_train_bert)\n","X_test_bert = scaler.transform(X_test_bert)\n","\n","knn_clf = KNeighborsClassifier(n_neighbors=5)\n","knn_clf.fit(X_train_bert, y_train)\n","\n","# Make predictions\n","y_pred = knn_clf.predict(X_test_bert)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n"],"metadata":{"id":"P5onPYRCz9GQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693379007140,"user_tz":-330,"elapsed":596,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"f6a149ef-a2af-4165-975b-346127c62a2c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.6262135922330098\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:187: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n","  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.preprocessing import MinMaxScaler #fixed import\n","scaler = MinMaxScaler()\n","X_train_bert = scaler.fit_transform(X_train_bert)\n","X_test_bert = scaler.transform(X_test_bert)\n","# scaler = StandardScaler()\n","# X_train_bert = scaler.fit_transform(X_train_bert)\n","# X_test_bert = scaler.transform(X_test_bert)\n","\n","mnb_clf = MultinomialNB()\n","mnb_clf.fit(X_train_bert, y_train)\n","\n","# Make predictions\n","y_pred = mnb_clf.predict(X_test_bert)\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n"],"metadata":{"id":"qfu4a9wp0DEv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693379011643,"user_tz":-330,"elapsed":408,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"cd467c25-a3af-4185-b7ae-6a24a146a31d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.6844660194174758\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["from sklearn.ensemble import VotingClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","# Create a VotingClassifier with your base models\n","ensemble_model = VotingClassifier(estimators=[\n","    ('model1', svm_clf),\n","    ('model2', mnb_clf),\n","    ('model3', knn_clf),\n","    ('model4', SVC_clf),\n","    ('model5', RF_clf),\n","    ('model6', DT_clf),\n","    ('model7', LR_clf),\n","\n","    # Add more base models as needed\n","], voting='hard')  # 'soft' for weighted voting based on confidence scores\n","\n","# Fit the ensemble model on the training data\n","ensemble_model.fit(X_train_bert, y_train)\n","# Make predictions with the ensemble model\n","ensemble_predictions = ensemble_model.predict(X_test_bert)\n","\n","# Fit the ensemble model on the training data\n","ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n","ensemble_report = classification_report(y_test, ensemble_predictions)\n","\n","print(f\"Ensemble Model Accuracy: {ensemble_accuracy}\")\n","print(\"Ensemble Model Classification Report:\")\n","print(ensemble_report)\n","\n"],"metadata":{"id":"6Y6RVN-fgTZt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693379254260,"user_tz":-330,"elapsed":24330,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"fb9276a4-c6e0-4b80-e4e0-fbdb13382f5f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:187: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n","  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Ensemble Model Accuracy: 0.7427184466019418\n","Ensemble Model Classification Report:\n","                   precision    recall  f1-score   support\n","\n","        Hand_over       0.00      0.00      0.00         2\n","  about_onpassive       0.67      0.67      0.67         3\n","  ask_botcreation       1.00      1.00      1.00         1\n","          ask_fyn       0.00      0.00      0.00         1\n","       ask_howold       1.00      1.00      1.00         3\n","        ask_howru       0.60      0.86      0.71         7\n","        ask_isbot       0.67      0.29      0.40         7\n","      ask_ishuman       0.33      0.25      0.29         4\n"," ask_languagesbot       1.00      1.00      1.00        11\n","ask_whatspossible       0.67      0.50      0.57         4\n","        assist_me       0.00      0.00      0.00         4\n","  contact_details       0.88      1.00      0.94        15\n"," customer_support       1.00      0.33      0.50         3\n","          founder       0.83      0.62      0.71         8\n","     good_evening       1.00      1.00      1.00         1\n","     good_morning       0.50      0.50      0.50         2\n","       good_night       0.00      0.00      0.00         2\n","          goodbye       0.67      0.50      0.57         4\n","            greet       0.83      0.42      0.56        12\n","         location       1.00      1.00      1.00         4\n","onpassive_profile       0.00      0.00      0.00         3\n","  onpassive_sucks       1.00      0.67      0.80         3\n","     out_of_scope       0.68      0.95      0.79        83\n","   react_positive       1.00      0.33      0.50         6\n"," support_feedback       1.00      0.33      0.50         3\n","            thank       1.00      0.70      0.82        10\n","\n","         accuracy                           0.74       206\n","        macro avg       0.67      0.54      0.57       206\n","     weighted avg       0.73      0.74      0.70       206\n","\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","def preprocess_new_data(new_data):\n","    # Initialize stopwords\n","    stop_words = set(stopwords.words('english'))\n","\n","    preprocessed_data = []\n","\n","    for text in new_data:\n","        # Tokenize the text\n","        tokens = word_tokenize(text)\n","\n","        # Remove punctuation and convert to lowercase\n","        tokens = [word.lower() for word in tokens if word.isalnum()]\n","\n","        # Remove stopwords\n","        tokens = [word for word in tokens if word not in stop_words]\n","\n","        # Join tokens back into a string\n","        preprocessed_text = ' '.join(tokens)\n","\n","        preprocessed_data.append(preprocessed_text)\n","\n","    return preprocessed_data\n","\n","# Example usage:\n","new_data = [\"how to delete the profile\"]\n","preprocessed_data = preprocess_new_data(new_data)\n","print(preprocessed_data)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZztQorURQVP","executionInfo":{"status":"ok","timestamp":1693394437715,"user_tz":-330,"elapsed":1480,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"a9d04234-aad7-4e53-d2e9-d930c25df4be"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["['delete profile']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["X_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xsBSmrx1Yj5k","executionInfo":{"status":"ok","timestamp":1693381185273,"user_tz":-330,"elapsed":13,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"be42a855-1ed2-495c-d416-96fa77d359fc"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1371                        The Try it out is not working\n","267                      Is it a multi lingual supportive\n","1063                                Is Rasa really smart?\n","587                                           I am hungry\n","1629                      I want to speak binary with you\n","                              ...                        \n","347     i have an issue, navigate me to the compliant ...\n","214                                              hello gm\n","584                      tell me whereabouts of onpassive\n","121                       I need help to solve my queries\n","7251                                         what's 5 + 5\n","Name: text, Length: 821, dtype: object"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["# Assuming you've already trained and defined your ensemble model\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import re\n","# Create the TF-IDF vectorizer with the same parameters as used in training\n","tfidf_vectorizer = TfidfVectorizer(\n","    max_features=1000,  # Adjust this based on your training data\n","    stop_words='english',  # Or use the same stop words as during training\n","    ngram_range=(1, 2)  # Use the same ngram range as during training\n",")\n","\n","def preprocess_text(text):\n","    # Remove special characters and digits\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Lemmatize the words\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Rejoin the tokens into a single string\n","    preprocessed_text = ' '.join(tokens)\n","\n","    return preprocessed_text\n","\n","# Fit the TF-IDF vectorizer on your training data (only once, not on new data)\n","# X_train_bert_processed = preprocess_text(X_train)  # Replace with your preprocessing function\n","# tfidf_vectorizer.fit(X_train_bert_processed)\n","\n","# Preprocess the new text data (replace this with your actual preprocessing steps)\n","new_text_data = \"are you ok\"\n","new_text_data_processed = preprocess_text(new_text_data)  # Replace with your preprocessing function\n","print(new_text_data_processed,\"prinitn\")\n","# Transform the preprocessed text data using the fitted TF-IDF vectorizer\n","# new_text_data_vectorized = tfidf_vectorizer.transform(new_text_data_processed)\n","\n","# Make predictions using the ensemble model\n","ensemble_predictions = ensemble_model.predict(new_text_data_vectorized)\n","\n","# Print the predictions\n","print(ensemble_predictions)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"4JYAD7uhWi80","executionInfo":{"status":"error","timestamp":1693394394608,"user_tz":-330,"elapsed":385,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"9c3e50fc-9c33-473a-acd7-daa679587670"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-5ee629ee03f2>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Preprocess the new text data (replace this with your actual preprocessing steps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mnew_text_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"are you ok\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mnew_text_data_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace with your preprocessing function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_text_data_processed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"prinitn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Transform the preprocessed text data using the fitted TF-IDF vectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-5ee629ee03f2>\u001b[0m in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Tokenize the text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"]}]},{"cell_type":"code","source":["# import xgboost as xgb\n","# scaler = StandardScaler()\n","# X_train_bert = scaler.fit_transform(X_train_bert)\n","# X_test_bert = scaler.transform(X_test_bert)\n","# from sklearn.preprocessing import LabelEncoder\n","# le = LabelEncoder()\n","# y_train = le.fit_transform(y_train)\n","\n","# xgb_clf = xgb.XGBClassifier()\n","# xgb_clf.fit(X_train_bert, y_train)\n","\n","# # Make predictions\n","# y_pred = xgb_clf.predict(X_test_bert)\n","\n","# # Evaluate the model\n","# accuracy = accuracy_score(y_test, y_pred)\n","# report = classification_report(y_test, y_pred)\n","\n","# print(f\"Accuracy: {accuracy}\")"],"metadata":{"id":"-PX3Qc8T0HzZ","executionInfo":{"status":"aborted","timestamp":1693378818378,"user_tz":-330,"elapsed":25,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y_train = le.fit_transform(y_train)\n","\n","clf.fit(X_train_bert, y_train)\n","\n","# Make predictions\n","y_pred = clf.predict(X_test_bert)\n","\n","# Create a simple neural network model\n","model = keras.Sequential([\n","    keras.layers.Dense(128, activation='relu', input_shape=(X_train_bert.shape[1],)),\n","    keras.layers.Dense(64, activation='relu'),\n","    keras.layers.Dense(350, activation='softmax')  # num_classes is the number of classes in your problem\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train_bert, y_train, epochs=10, batch_size=32, verbose=2)\n","\n","# Evaluate the model\n","y_pred = model.predict_classes(X_test_bert)\n","accuracy = accuracy_score(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","print(f\"Accuracy: {accuracy}\")\n","print(\"Classification Report:\")\n","print(report)\n","#Please note that you'll need to customize these code templates by loading and preprocessing your specific dataset. Additionally, you may need to adjust hyperparameters and model architectures to suit your problem's requirements."],"metadata":{"id":"4xe-7vsy0Y-g","colab":{"base_uri":"https://localhost:8080/","height":620},"executionInfo":{"status":"error","timestamp":1693379386124,"user_tz":-330,"elapsed":4443,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}},"outputId":"6e8f844a-2b6c-499b-b708-50025fc60c72"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","26/26 - 2s - loss: 3.7954 - accuracy: 0.3886 - 2s/epoch - 86ms/step\n","Epoch 2/10\n","26/26 - 0s - loss: 2.6442 - accuracy: 0.4166 - 158ms/epoch - 6ms/step\n","Epoch 3/10\n","26/26 - 0s - loss: 2.4824 - accuracy: 0.4166 - 151ms/epoch - 6ms/step\n","Epoch 4/10\n","26/26 - 0s - loss: 2.4359 - accuracy: 0.4166 - 194ms/epoch - 7ms/step\n","Epoch 5/10\n","26/26 - 0s - loss: 2.3708 - accuracy: 0.4166 - 198ms/epoch - 8ms/step\n","Epoch 6/10\n","26/26 - 0s - loss: 2.2885 - accuracy: 0.4397 - 155ms/epoch - 6ms/step\n","Epoch 7/10\n","26/26 - 0s - loss: 2.2022 - accuracy: 0.4543 - 154ms/epoch - 6ms/step\n","Epoch 8/10\n","26/26 - 0s - loss: 2.1289 - accuracy: 0.4787 - 150ms/epoch - 6ms/step\n","Epoch 9/10\n","26/26 - 0s - loss: 2.0015 - accuracy: 0.4921 - 145ms/epoch - 6ms/step\n","Epoch 10/10\n","26/26 - 0s - loss: 1.9207 - accuracy: 0.5043 - 172ms/epoch - 7ms/step\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-3584cd79ed75>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_bert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_classes'"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n","\n","# Assuming you have already made predictions and stored them in y_pred\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy}')\n","\n","# Calculate precision, recall, and F1-score\n","precision = precision_score(y_test, y_pred, average='weighted')\n","recall = recall_score(y_test, y_pred, average='weighted')\n","f1 = f1_score(y_test, y_pred, average='weighted')\n","\n","print(f'Precision: {precision}')\n","print(f'Recall: {recall}')\n","print(f'F1-score: {f1}')\n","\n","# Generate a classification report with precision, recall, and F1-score for each class\n","report = classification_report(y_test, y_pred)\n","print(report)\n"],"metadata":{"id":"6zlhtB_RxkUL","executionInfo":{"status":"aborted","timestamp":1693378818379,"user_tz":-330,"elapsed":25,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocess the new text\n","new_text = \"Can you explain me in one sentence what you are doing?\"\n","new_text_preprocessed = preprocess_text(new_text)  # Apply the same preprocessing steps\n","\n","# Feature extraction using the same TF-IDF vectorizer\n","new_text_tfidf = tfidf_vectorizer.transform([new_text_preprocessed])\n","\n","# Predict the intent label\n","predicted_intent = clf.predict(new_text_tfidf)[0]\n","\n","print(f\"Predicted Intent: {predicted_intent}\")"],"metadata":{"id":"0vLpHxzSx9bk","executionInfo":{"status":"aborted","timestamp":1693378818380,"user_tz":-330,"elapsed":26,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tLrwlGlAz0MD","executionInfo":{"status":"aborted","timestamp":1693378818381,"user_tz":-330,"elapsed":26,"user":{"displayName":"Ravi Kumar","userId":"12176873388585538917"}}},"execution_count":null,"outputs":[]}]}